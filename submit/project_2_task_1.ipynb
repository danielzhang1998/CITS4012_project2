{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CITS4012 Project2\n",
    "\n",
    "Theo Andily  22764884\n",
    "\n",
    "Hanlin Zhang 22541459"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader as api\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import push_notebook, output_notebook\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "\n",
    "import gensim\n",
    "import re\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import timeit\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import en_core_web_sm\n",
    "import os\n",
    "\n",
    "from argparse import Namespace\n",
    "import collections\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    seed=1337,\n",
    "    output=\"output.csv\",\n",
    "    train_proportion=0.7,\n",
    "    val_proportion=0.1,\n",
    "    test_proportion=0.2,\n",
    "    domain_model_dir=\"model_storage/domain_model/\",\n",
    "    domain_file_name=\"domain.model\",\n",
    "    expand_filepaths_to_save_dir=True\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_csv(csv_file):\n",
    "    return pd.read_csv(csv_file)\n",
    "\n",
    "job_data = load_csv(\"seek_australia.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theor\\AppData\\Local\\Temp\\ipykernel_7976\\154546671.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  job_data['job_description_clean'] = job_data['job_description_clean'].str.replace(r'[^\\w\\s]+', '')\n"
     ]
    }
   ],
   "source": [
    "# try to remove the broken characters\n",
    "\n",
    "for each in ['š', 'ª', 'º', 'Ÿ', 'µ', 'ˆ', 'œ', 'é', 'è', 'ï', 'Š', 'â', 'ç', 'æ', 'å', 'ä', 'ð', \n",
    "            'Ã', 'Â', 'Å', 'Ä', '®', '€', '™', 'ž', 'œ', '¼', '½', '¾', '¹', '²','³', '\\xa0']:\n",
    "    try:\n",
    "        job_data['job_description_clean'] = job_data['job_description_clean'].replace({each:''}, regex=True)\n",
    "    except:\n",
    "        job_data['job_description_clean'] = job_data['job_description'].replace({each:''}, regex=True)\n",
    "\n",
    "job_data['job_description_clean'] = job_data['job_description_clean'].str.replace(r'[^\\w\\s]+', '')\n",
    "\n",
    "job_data['job_type'] = np.where(job_data['job_type'] == 'Full Time', 'Full Time', 'Other')  # replace the job type to be Other except Full Time\n",
    "\n",
    "job_data = job_data[job_data['job_description_clean'].notna()]  # keep the job data sample which is not na only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we removed unnessaary unicode characters, punctuation and change the job type to either be full time or other. The way we remove punctuation could join two or more words together and therefore loses its sematic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get job description from the data set and get random 2000 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate the tfidf of each category which runs over all the job description and hence takes a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_pipe(original_text: list):\n",
    "    stopword = stopwords.words('english')\n",
    "    \"\"\" This function returns the featured words in after tfidf up to 500 words \"\"\"\n",
    "    if len(original_text) > 1 and original_text != np.nan:\n",
    "        tfidf_vectorizer = TfidfVectorizer(smooth_idf=True)\n",
    "        tfidf = tfidf_vectorizer.fit_transform(original_text)\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        tfidf_values = np.argsort(tfidf.toarray()).flatten()[::-1]\n",
    "        tfidf_return = []\n",
    "        counter = 0\n",
    "        while len(tfidf_return) < 10:\n",
    "            if feature_names[tfidf_values].tolist()[counter] not in stopword and feature_names[tfidf_values].tolist()[counter].isdigit() != True:\n",
    "                tfidf_return.append(feature_names[tfidf_values].tolist()[counter])\n",
    "            counter += 1\n",
    "        return tfidf_return\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed tfidf for Retail & Consumer Products time taken:  6.132s\n",
      "Completed tfidf for Government & Defence time taken:  11.673s\n",
      "Completed tfidf for Trades & Services time taken:  40.048s\n",
      "Completed tfidf for Manufacturing, Transport & Logistics time taken:  21.966s\n",
      "Completed tfidf for Sales time taken:  12.719s\n",
      "Completed tfidf for Community Services & Development time taken:  5.539s\n",
      "Completed tfidf for Healthcare & Medical time taken:  31.033s\n",
      "Completed tfidf for Information & Communication Technology time taken:  33.995s\n",
      "Completed tfidf for Mining, Resources & Energy time taken:  4.935s\n",
      "Completed tfidf for Construction time taken:  9.654s\n",
      "Completed tfidf for Design & Architecture time taken:  0.899s\n",
      "Completed tfidf for Call Centre & Customer Service time taken:  3.614s\n",
      "Completed tfidf for Marketing & Communications time taken:  4.119s\n",
      "Completed tfidf for Administration & Office Support time taken:  14.234s\n",
      "Completed tfidf for Banking & Financial Services time taken:  3.373s\n",
      "Completed tfidf for Engineering time taken:  5.826s\n",
      "Completed tfidf for Education & Training time taken:  11.286s\n",
      "Completed tfidf for Accounting time taken:  11.981s\n",
      "Completed tfidf for Human Resources & Recruitment time taken:  3.482s\n",
      "Completed tfidf for Real Estate & Property time taken:  2.508s\n",
      "Completed tfidf for Sport & Recreation time taken:  0.307s\n",
      "Completed tfidf for Legal time taken:  2.182s\n",
      "Completed tfidf for Hospitality & Tourism time taken:  12.465s\n",
      "Completed tfidf for Consulting & Strategy time taken:  0.488s\n",
      "Completed tfidf for Farming, Animals & Conservation time taken:  0.22s\n",
      "Completed tfidf for Advertising, Arts & Media time taken:  0.41s\n",
      "Completed tfidf for Insurance & Superannuation time taken:  0.74s\n",
      "Completed tfidf for Self Employment time taken:  0.013s\n",
      "Completed tfidf for CEO & General Management time taken:  0.194s\n",
      "Completed tfidf for Science & Technology time taken:  0.463s\n"
     ]
    }
   ],
   "source": [
    "job_description_per_category = {}\n",
    "tfidf_job_category = {}\n",
    "unique_category=job_data['category'].unique()\n",
    "for jobs in unique_category:\n",
    "    start = timeit.default_timer()\n",
    "    tfidf_values = tfidf_pipe(job_data.loc[job_data['category'] == jobs]['job_description_clean'].tolist())\n",
    "    stop = timeit.default_timer()\n",
    "    tfidf_job_category[jobs] = tfidf_values\n",
    "    print('Completed tfidf for', jobs, 'time taken: ', str(round((stop - start), 3)) +'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data.head()\n",
    "job_data['tfidf'] = ''\n",
    "for jobs in unique_category:\n",
    "    job_data.loc[job_data['category'] == jobs, 'tfidf'] = [' '.join(tfidf_job_category[jobs])] * len(job_data.loc[job_data['category'] == jobs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = job_data[['job_description_clean', 'job_type', 'category', 'tfidf']] # keep column with name job_description_clean, job_type, category and tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we remove the useless columns (retain job_description, job_type, category and tfidf top 10 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.to_csv('cleaned_data_new.csv', index=False)    # store the result into the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train by category\n",
    "# Create dict\n",
    "by_category = collections.defaultdict(list)\n",
    "for _, row in cleaned_data.iterrows():\n",
    "    by_category[row.job_type].append(row.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create split data\n",
    "final_list = []\n",
    "np.random.seed(args.seed)\n",
    "for _, item_list in sorted(by_category.items()):\n",
    "    np.random.shuffle(item_list)\n",
    "    n = len(item_list)\n",
    "    n_train = int(args.train_proportion*n)\n",
    "    n_val = int(args.val_proportion*n)\n",
    "    n_test = int(args.test_proportion*n)\n",
    "    \n",
    "    # Give data point a split attribute\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'\n",
    "    for item in item_list[n_train:n_train+n_val]:\n",
    "        item['split'] = 'val'\n",
    "    for item in item_list[n_train+n_val:]:\n",
    "        item['split'] = 'test'  \n",
    "    \n",
    "    # Add to final list\n",
    "    final_list.extend(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write split data to file\n",
    "final_data = pd.DataFrame(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    20758\n",
       "test      5932\n",
       "val       2965\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean sentences\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "\n",
    "final_data.job_description_clean = final_data.job_description_clean.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_description_clean</th>\n",
       "      <th>job_type</th>\n",
       "      <th>category</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ph super uncapped commission overtime availab...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Sales</td>\n",
       "      <td>startrack collate freight analyst post unrival...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>universal recruitment has years of experience...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Manufacturing, Transport &amp; Logistics</td>\n",
       "      <td>controller malec bros fleet multitasking movem...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goodlife health clubsis the largest australia...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Call Centre &amp; Customer Service</td>\n",
       "      <td>service desk consultant customer incidents tec...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the company our client has a very dynamic and...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Call Centre &amp; Customer Service</td>\n",
       "      <td>service desk consultant customer incidents tec...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>about the business we are a small family busi...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>Trades &amp; Services</td>\n",
       "      <td>pest anticimex flick control australias hygien...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               job_description_clean   job_type  \\\n",
       "0   ph super uncapped commission overtime availab...  Full Time   \n",
       "1   universal recruitment has years of experience...  Full Time   \n",
       "2   goodlife health clubsis the largest australia...  Full Time   \n",
       "3   the company our client has a very dynamic and...  Full Time   \n",
       "4   about the business we are a small family busi...  Full Time   \n",
       "\n",
       "                               category  \\\n",
       "0                                 Sales   \n",
       "1  Manufacturing, Transport & Logistics   \n",
       "2        Call Centre & Customer Service   \n",
       "3        Call Centre & Customer Service   \n",
       "4                     Trades & Services   \n",
       "\n",
       "                                               tfidf  split  \n",
       "0  startrack collate freight analyst post unrival...  train  \n",
       "1  controller malec bros fleet multitasking movem...  train  \n",
       "2  service desk consultant customer incidents tec...  train  \n",
       "3  service desk consultant customer incidents tec...  train  \n",
       "4  pest anticimex flick control australias hygien...  train  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have a quick look how the data frame looks like\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write munged data to CSV\n",
    "final_data.to_csv(args.output, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_tsne(text_labels, tsne_array):\n",
    "    '''makes an interactive scatter plot with text labels for each point'''\n",
    "\n",
    "    # Define a dataframe to be used by bokeh context\n",
    "    bokeh_df = pd.DataFrame(tsne_array, text_labels, columns=['x','y'])\n",
    "    bokeh_df['text_labels'] = bokeh_df.index\n",
    "\n",
    "    # interactive controls to include to the plot\n",
    "    TOOLS=\"hover, zoom_in, zoom_out, box_zoom, undo, redo, reset, box_select\"\n",
    "\n",
    "    p = figure(tools=TOOLS, plot_width=700, plot_height=700)\n",
    "\n",
    "    # define data source for the plot\n",
    "    source = ColumnDataSource(bokeh_df)\n",
    "\n",
    "    # scatter plot\n",
    "    p.scatter('x', 'y', source=source, fill_alpha=0.6,\n",
    "              fill_color=\"#8724B5\",\n",
    "              line_color=None)\n",
    "\n",
    "    # text labels\n",
    "    labels = LabelSet(x='x', y='y', text='text_labels', y_offset=8,\n",
    "                      text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                      source=source, text_align='center')\n",
    "\n",
    "    p.add_layout(labels)\n",
    "\n",
    "    # show plot inline\n",
    "    output_notebook()\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(tup, dict):\n",
    "    for a, b in tup:\n",
    "        dict[a] = b\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we merge all the job description data from the sample in train data set into one str, then do some simple preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = final_data[final_data['split'] == 'train']\n",
    "job_descript_str = ' '.join(train_df['job_description_clean'])\n",
    "doc_tokenized = gensim.utils.simple_preprocess(str(job_descript_str), deacc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary()\n",
    "BoW_corpus = dictionary.doc2bow(doc_tokenized, allow_update=True)\n",
    "BoW_corpus = [(dictionary[id], freq) for id, freq in BoW_corpus]\n",
    "\n",
    "BoW_corpus_dict = dict()\n",
    "convert_result = convert(BoW_corpus, BoW_corpus_dict)  # Here is a varible placement just to remove the cell from printing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build and train the domain specific embeddings from the word2vec library with the bag of word frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = 16\n",
    "model = Word2Vec(min_count=3,\n",
    "                     window=2,\n",
    "                     vector_size=100,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.05, \n",
    "                     min_alpha=0.0005, \n",
    "                     negative=15,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.02 mins\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t = time()\n",
    "\n",
    "model.build_vocab([doc_tokenized], progress_per=10)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.domain_model = os.path.join(args.domain_model_dir,\n",
    "                                        args.domain_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If model exists, read it. Otherwise train it.\n",
    "if os.path.exists(args.domain_model_dir):\n",
    "    model = Word2Vec.load(args.domain_model)\n",
    "else:\n",
    "    t = time()\n",
    "    model.train(doc_tokenized, total_examples=model.corpus_count, epochs=100, report_delay=1)\n",
    "    print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "    os.makedirs(args.domain_model_dir)\n",
    "    model.save(args.domain_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better visualization, it is better to plot some of words, instead of all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\envs\\cits4012\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "c:\\envs\\cits4012\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1165\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  const JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n    // Clean up Bokeh references\n    if (id != null && id in Bokeh.index) {\n      Bokeh.index[id].model.document.clear();\n      delete Bokeh.index[id];\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim();\n            if (id in Bokeh.index) {\n              Bokeh.index[id].model.document.clear();\n              delete Bokeh.index[id];\n            }\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1165\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.2.min.js\"];\n  const css_urls = [];\n  \n\n  const inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1165\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"cff71a87-efa8-41ab-b2be-5f7c72a54437\" data-root-id=\"1120\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function embed_document(root) {\n    \n  const docs_json = {\"0461a234-4868-4ed9-bb64-88be9a3aa06c\":{\"defs\":[],\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1129\"}],\"center\":[{\"id\":\"1132\"},{\"id\":\"1136\"},{\"id\":\"1163\"}],\"height\":700,\"left\":[{\"id\":\"1133\"}],\"renderers\":[{\"id\":\"1161\"}],\"title\":{\"id\":\"1178\"},\"toolbar\":{\"id\":\"1147\"},\"width\":700,\"x_range\":{\"id\":\"1121\"},\"x_scale\":{\"id\":\"1125\"},\"y_range\":{\"id\":\"1123\"},\"y_scale\":{\"id\":\"1127\"}},\"id\":\"1120\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"coordinates\":null,\"group\":null,\"source\":{\"id\":\"1156\"},\"text\":{\"field\":\"text_labels\"},\"text_align\":{\"value\":\"center\"},\"text_color\":{\"value\":\"#555555\"},\"text_font_size\":{\"value\":\"8pt\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"},\"y_offset\":{\"value\":8}},\"id\":\"1163\",\"type\":\"LabelSet\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.2},\"fill_color\":{\"value\":\"#8724B5\"},\"hatch_alpha\":{\"value\":0.2},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"value\":null},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1160\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"1123\",\"type\":\"DataRange1d\"},{\"attributes\":{\"source\":{\"id\":\"1156\"}},\"id\":\"1162\",\"type\":\"CDSView\"},{\"attributes\":{\"overlay\":{\"id\":\"1146\"}},\"id\":\"1140\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1121\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1139\",\"type\":\"ZoomOutTool\"},{\"attributes\":{},\"id\":\"1127\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1182\",\"type\":\"AllLabels\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"1156\"},\"glyph\":{\"id\":\"1158\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"1160\"},\"nonselection_glyph\":{\"id\":\"1159\"},\"view\":{\"id\":\"1162\"}},\"id\":\"1161\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1130\",\"type\":\"BasicTicker\"},{\"attributes\":{\"axis\":{\"id\":\"1133\"},\"coordinates\":null,\"dimension\":1,\"group\":null,\"ticker\":null},\"id\":\"1136\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1185\",\"type\":\"AllLabels\"},{\"attributes\":{},\"id\":\"1181\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"coordinates\":null,\"formatter\":{\"id\":\"1184\"},\"group\":null,\"major_label_policy\":{\"id\":\"1185\"},\"ticker\":{\"id\":\"1130\"}},\"id\":\"1129\",\"type\":\"LinearAxis\"},{\"attributes\":{\"callback\":null},\"id\":\"1137\",\"type\":\"HoverTool\"},{\"attributes\":{\"bottom_units\":\"screen\",\"coordinates\":null,\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"group\":null,\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"syncable\":false,\"top_units\":\"screen\"},\"id\":\"1145\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"1186\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"axis\":{\"id\":\"1129\"},\"coordinates\":null,\"group\":null,\"ticker\":null},\"id\":\"1132\",\"type\":\"Grid\"},{\"attributes\":{\"tools\":[{\"id\":\"1137\"},{\"id\":\"1138\"},{\"id\":\"1139\"},{\"id\":\"1140\"},{\"id\":\"1141\"},{\"id\":\"1142\"},{\"id\":\"1143\"},{\"id\":\"1144\"}]},\"id\":\"1147\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1125\",\"type\":\"LinearScale\"},{\"attributes\":{\"coordinates\":null,\"group\":null},\"id\":\"1178\",\"type\":\"Title\"},{\"attributes\":{\"coordinates\":null,\"formatter\":{\"id\":\"1181\"},\"group\":null,\"major_label_policy\":{\"id\":\"1182\"},\"ticker\":{\"id\":\"1134\"}},\"id\":\"1133\",\"type\":\"LinearAxis\"},{\"attributes\":{\"data\":{\"index\":[\"experience\",\"work\",\"role\",\"team\",\"skills\",\"business\",\"working\",\"apply\",\"management\",\"please\",\"ability\",\"service\",\"opportunity\",\"support\",\"position\",\"services\",\"within\",\"strong\",\"people\",\"environment\",\"customer\",\"development\",\"new\",\"including\",\"company\",\"high\",\"successful\",\"required\",\"looking\",\"health\",\"communication\",\"excellent\",\"provide\",\"must\",\"join\",\"client\",\"across\",\"time\",\"sales\",\"career\",\"project\",\"training\",\"industry\",\"clients\",\"years\",\"part\",\"manager\",\"well\",\"knowledge\",\"quality\",\"information\",\"australia\",\"ensure\",\"opportunities\",\"professional\",\"staff\",\"key\",\"care\",\"based\",\"experienced\",\"great\",\"projects\",\"current\",\"applications\",\"range\",\"need\",\"highly\",\"include\",\"contact\",\"customers\"],\"text_labels\":[\"experience\",\"work\",\"role\",\"team\",\"skills\",\"business\",\"working\",\"apply\",\"management\",\"please\",\"ability\",\"service\",\"opportunity\",\"support\",\"position\",\"services\",\"within\",\"strong\",\"people\",\"environment\",\"customer\",\"development\",\"new\",\"including\",\"company\",\"high\",\"successful\",\"required\",\"looking\",\"health\",\"communication\",\"excellent\",\"provide\",\"must\",\"join\",\"client\",\"across\",\"time\",\"sales\",\"career\",\"project\",\"training\",\"industry\",\"clients\",\"years\",\"part\",\"manager\",\"well\",\"knowledge\",\"quality\",\"information\",\"australia\",\"ensure\",\"opportunities\",\"professional\",\"staff\",\"key\",\"care\",\"based\",\"experienced\",\"great\",\"projects\",\"current\",\"applications\",\"range\",\"need\",\"highly\",\"include\",\"contact\",\"customers\"],\"x\":{\"__ndarray__\":\"q1s1wZyX8EFpGmPBMNbvwEfxTj9fAPc+Ewv4QHKTq8E3Ny7B+DHxQUb2VUF+i+9BL76wwXrRVcF2kZtBe7bXwdAJaEGDQwfCD8amwe/GAcI3lvK9i2myQZqe+cA4oARAy6s2wcPbRkFuQo9BBkejwaCY8cE5zF5B7/T1QVEzssHK1jvAN1IjQVSCBMGMMlPBJv57wM2dij+V523BNX1+wZcKDMKEVWO9Xd7pwfawEEEYmcVBEMqQQcre5UCVCADBZNVlQMJYI0EvM9TBjdYQwl4MIMJjVcNAsMOdQfePwsHXU6bAp0z+wEFFgUGdpcy+5dHAQCvzVUG/cETBSeXKQYo2KL/Ow4lAaJeJQaTwvEEHMp/BuASxQA==\",\"dtype\":\"float32\",\"order\":\"little\",\"shape\":[70]},\"y\":{\"__ndarray__\":\"06XuwPKEGkHeGMRA/2TbQUTIkMHyz7hBUGnCQczaCMHkIwJA8yGUQaNB9sDETFi/kjukQbNdv8FJh1rA4BJjwWLTYEH2F7/Buv/AwViix0AO+UrA6BtTQKx9O8FskPhBXjFhQfTLBEJHwqfB0MTVPtYVTEEZwAnCODCJwUXE+EHu44lAbC3fQBpaoMHhhLNBZMyEQeiHAMJznE7AqRZVwbx4KT4OJxxBh5DpwPGflsFC1w7BUVqsQUVjMkFqAPnBSH41QMYBdr8oOizAt4kNwV5xCUEQakjBkfwVQSmdwEDsSFXA3WIBQRB3RUDKMEDBU9mQQTH+GEKfEwpCeTLdQbuNKkKor+DBjrxlwfD1YUG+YDJBO8HBwA==\",\"dtype\":\"float32\",\"order\":\"little\",\"shape\":[70]}},\"selected\":{\"id\":\"1187\"},\"selection_policy\":{\"id\":\"1186\"}},\"id\":\"1156\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1184\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1141\",\"type\":\"UndoTool\"},{\"attributes\":{},\"id\":\"1187\",\"type\":\"Selection\"},{\"attributes\":{\"bottom_units\":\"screen\",\"coordinates\":null,\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"group\":null,\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"syncable\":false,\"top_units\":\"screen\"},\"id\":\"1146\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"1142\",\"type\":\"RedoTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#8724B5\"},\"hatch_alpha\":{\"value\":0.1},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":null},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1159\",\"type\":\"Scatter\"},{\"attributes\":{\"overlay\":{\"id\":\"1145\"}},\"id\":\"1144\",\"type\":\"BoxSelectTool\"},{\"attributes\":{},\"id\":\"1138\",\"type\":\"ZoomInTool\"},{\"attributes\":{},\"id\":\"1134\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1143\",\"type\":\"ResetTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.6},\"fill_color\":{\"value\":\"#8724B5\"},\"line_color\":{\"value\":null},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1158\",\"type\":\"Scatter\"}],\"root_ids\":[\"1120\"]},\"title\":\"Bokeh Application\",\"version\":\"2.4.2\"}};\n  const render_items = [{\"docid\":\"0461a234-4868-4ed9-bb64-88be9a3aa06c\",\"root_ids\":[\"1120\"],\"roots\":{\"1120\":\"cff71a87-efa8-41ab-b2be-5f7c72a54437\"}}];\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1120"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_sorted = dict(sorted(BoW_corpus_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "input_vocab = []\n",
    "input_vocab =  [word for word in vocab_sorted if word in model.wv.key_to_index.keys() and word not in stopwords_list]\n",
    "points = len(input_vocab)\n",
    "X = model.wv[input_vocab]\n",
    "X_tsne = TSNE(n_components=2, random_state=4012).fit_transform(X[:70])\n",
    "\n",
    "interactive_tsne(list(input_vocab)[:70], X_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plotted the first 70 words in our input vocab from the Word2Vec model that we trained and plotted them using the TSNE plotter. The plot above shows the similarity of the words by their distance from neighbouring words. For example, the word provide is simialr to training, care, knowledge, customer key and management based on the word vectors but they are not a good representation since the distance between excellent is far from great and successful. This is due to the domain speicific nature of the word vectoriser since it was only given the job description as training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN model using the top 10 tfidf words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section we build a feed forwards neural network using the top 10 tfidf words with one hidden layer and we followed closely to the codes on the labs and modified to our need for tfidf. The fnn only used the onehotencoder vectoriser which is implemented from the lab and the use of onehotencoder is not very efficient given their nature of high feature size as it builds one vector per unique word. The training time will depends on the number of epochs but we kept a 100 epochs for all of the models including in task 2, it should take a few minutes for ffnn, and cnn using tfidf but it will take a few hours for cnn with job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace(\n",
    "    # Data and path information\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file='model.pth',\n",
    "    tfidf_csv='output.csv',\n",
    "    save_dir='model_storage/fnn',\n",
    "    vectorizer_file='vectorizer.json',\n",
    "    # No model hyperparameters\n",
    "    # Training hyperparameters\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.0001,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    # Runtime options\n",
    "    cuda=True,\n",
    "    device='cuda',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tfidfDataset(Dataset):\n",
    "    def __init__(self, tfidf_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        tfidf_df (pandas.DataFrame): the dataset\n",
    "        vectorizer (tfidfVectorizer): vectorizer instantiated from dataset\n",
    "        \"\"\"\n",
    "        self.tfidf_df = tfidf_df\n",
    "        self._vectorizer = vectorizer\n",
    "        self.train_df = self.tfidf_df[self.tfidf_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        self.val_df = self.tfidf_df[self.tfidf_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "        self.test_df = self.tfidf_df[self.tfidf_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                            'val': (self.val_df, self.validation_size),\n",
    "                            'test': (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, tfidf_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        Args:\n",
    "            tfidf_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of tfidfDataset\n",
    "        \"\"\"\n",
    "        tfidf_df = pd.read_csv(tfidf_csv)\n",
    "        return cls(tfidf_df, tfidfVectorizer.from_dataframe(tfidf_df))\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe\n",
    "        Args:\n",
    "        split (str): one of \"train\", \"val\", or \"test\"\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        Args:\n",
    "        index (int): the index to the data point\n",
    "        Returns:\n",
    "        a dict of the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "        tfidf_vector = \\\n",
    "        self._vectorizer.vectorize(row.tfidf)\n",
    "        job_type_index = \\\n",
    "        self._vectorizer.job_type_vocab.lookup_token(row.job_type)\n",
    "        return {'x_data': tfidf_vector,\n",
    "                'y_target': job_type_index}\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        Args:\n",
    "        batch_size (int)\n",
    "        Returns:\n",
    "        number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract Vocabulary for mapping\"\"\"\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existingmap of tokens to indices\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token \n",
    "                                for token, idx in self._token_to_idx.items()}\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self.unk_index = 1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "    def to_serializable(self):\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx,\n",
    "                'add_unk': self._add_unk,\n",
    "                'unk_token': self._unk_token}\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token\n",
    "        or the UNK index if token isn't present.\n",
    "        Args:\n",
    "            token (str): the token to look up\n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary)\n",
    "            for the UNK functionality\n",
    "        \"\"\"\n",
    "        if self._add_unk:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        Args:\n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "        KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tfidfVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
    "    def __init__(self, tfidf_vocab, job_type_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tfidf_vocab (Vocabulary): maps words to integers\n",
    "            job_type_vocab (Vocabulary): maps type labels to integers\n",
    "        \"\"\"\n",
    "        self.tfidf_vocab = tfidf_vocab\n",
    "        self.job_type_vocab = job_type_vocab\n",
    "    def vectorize(self, tfidf):\n",
    "        \"\"\"Create a collapsed one hot vector for the tfidf\n",
    "        Args:\n",
    "            tfidf (str): the tfidf\n",
    "        Returns:\n",
    "            one_hot (np.ndarray): the collapsed onehot encoding\n",
    "        \"\"\"\n",
    "        one_hot = np.zeros(len(self.tfidf_vocab), dtype=np.float32)\n",
    "        for token in tfidf.split(\" \"):\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.tfidf_vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, tfidf_df, cutoff=25):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        Args:\n",
    "            tfidf_df (pandas.DataFrame): the tfidf dataset\n",
    "            cutoff (int): the parameter for frequency based filtering\n",
    "        Returns:\n",
    "            an instance of the tfidfVectorizer\n",
    "        \"\"\"\n",
    "        tfidf_vocab = Vocabulary(add_unk=True)\n",
    "        job_type_vocab = Vocabulary(add_unk=False)\n",
    "        # Add job_types\n",
    "        for job_type in sorted(set(tfidf_df.job_type)):\n",
    "            job_type_vocab.add_token(job_type)\n",
    "        # Add top words if count > provided count\n",
    "        word_counts = Counter()\n",
    "        for tfidf in tfidf_df.tfidf:\n",
    "            for word in tfidf.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                tfidf_vocab.add_token(word)\n",
    "        return cls(tfidf_vocab, job_type_vocab)\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"Intantiate a tfidfVectorizer from a serializable dictionary\n",
    "        Args:\n",
    "            contents (dict): the serializable dictionary\n",
    "        Returns:\n",
    "            an instance of the tfidfVectorizer class\n",
    "        \"\"\"\n",
    "        tfidf_vocab = Vocabulary.from_serializable(contents['tfidf_vocab'])\n",
    "        job_type_vocab = Vocabulary.from_serializable(contents['job_type_vocab'])\n",
    "        return cls(tfidf_vocab=tfidf_vocab, job_type_vocab=job_type_vocab)\n",
    "    def to_serializable(self):\n",
    "        \"\"\"Create the serializable dictionary for caching\n",
    "        Returns:\n",
    "            contents (dict): the serializable dictionary\n",
    "        \"\"\"\n",
    "        return {'tfidf_vocab': self.tfidf_vocab.to_serializable(),\n",
    "                'job_type_vocab': self.job_type_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will\n",
    "    ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, \n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tfidfClassifier(nn.Module):\n",
    "    \"\"\" a simple perceptron-based classifier \"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features (int): the size of the input feature vector\n",
    "        \"\"\"\n",
    "        super(tfidfClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features, out_features=1)\n",
    "    \n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor \n",
    "                    x_in.shape should be (batch, num_features)\n",
    "            apply_sigmoid (bool): a flag for the sigmoid activation\n",
    "                    should be false if used with the cross-entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch,).\n",
    "        \"\"\"\n",
    "        y_out = self.fc1(x_in).squeeze()\n",
    "        \n",
    "        if apply_sigmoid:\n",
    "            y_out = torch.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': 1,\n",
    "            'test_acc': 1}\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# dataset and vectorizer\n",
    "dataset = tfidfDataset.load_dataset_and_make_vectorizer(args.tfidf_csv)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "# model\n",
    "classifier = tfidfClassifier(num_features=len(vectorizer.tfidf_vocab))\n",
    "classifier = classifier.to(args.device)\n",
    "# loss and optimizer\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_index in range(args.num_epochs):\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "    # Iterate over training dataset\n",
    "    # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # the training routine is 5 steps:\n",
    "        # step 1. zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # step 2. compute the output\n",
    "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch-running_loss) / (batch_index + 1)\n",
    "        # step 4. use loss to produce gradients\n",
    "        loss.backward()\n",
    "        # step 5. use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "        # compute the accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "    # Iterate over val dataset\n",
    "    # setup: batch generator, set loss and acc to 0, set eval mode on\n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # step 1. compute the output\n",
    "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "        # step 2. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "        # step 3. compute the accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset,batch_size=args.batch_size,device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_batch = loss.item()\n",
    "    running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "    # compute the accuracy\n",
    "    acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.596\n",
      "Test Accuracy: 67.37\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried various parameters to find the best accuracy with the following parameters with three run time and average them, though given the nature of ffnn there will be different result but they will be close to the average.\n",
    "\n",
    "| batch_size, learning_rate, early_stopping_criteria | loss  | Accuracy |\n",
    "|----------------------------------------------------|-------|----------|\n",
    "| 128, 0.001, 5                                      | 0.596 | 67.19    |\n",
    "| 128, 0.01, 5                                       | 0.601 | 65.96    |\n",
    "| 64, 0.01, 5                                        | 0.608 | 65.15    |\n",
    "| 128, 0.0001, 5                                     | 0.597 | 67.31    |\n",
    "| 64, 0.0001, 5                                      | 0.598 | 67.24    |\n",
    "| 128, 0.0001, 8                                     | 0.597 | 67.27    |\n",
    "| 128, 0.0001, 3                                     | 0.597 | 67.24    |\n",
    "\n",
    "From the table above the biggest factor affecting the accuracy is the learning rate and the best set of parameters is with batch_size=128, learning_rate=0.0001, early_stopping_criteria=5. Though we did not consider adding more layers which could increase the perfomance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Conv1d based model using the top 10 tfidf words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of this section is to build a cnn model to classify the job type binary classification using the tfidf columns which was made in the data preprocessing and since the most of the tfidf values are similar per rows as they have the same category, this makes it harder for the model to classify the job types and should not give a higher probability per prediciton.\n",
    "\n",
    "To swap between the word embedding set the different args in the following cells:\n",
    "\n",
    "onehotencoder: use_glove=False, use_domain_specific=False,\n",
    "\n",
    "pretrained: use_glove=True, use_domain_specific=False\n",
    "\n",
    "domain_specific: use_glove=False, use_domain_specific=True\n",
    "\n",
    "The models are saved in the specified folder which are shown in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    data_set=\"cleaned_data_new.csv\",\n",
    "    vectorizer_file=\"cnn_vectorizer.json\",\n",
    "    model_state_file=\"cnn_model.pth\",\n",
    "    output=\"output.csv\",\n",
    "    save_dir_10_word_none_embedding=\"model_storage\\cnn_10_word_none_embedding\",\n",
    "    save_dir_10_words_pretrained=\"model_storage\\cnn_10_word_pretrained\",\n",
    "    save_dir_10_words_domain_speicific=\"model_storage\\cnn_10_word_domain_specific\",\n",
    "    domain_model_dir=\"model_storage\\domain_model\\domain.model\",\n",
    "    domain_file_name=\"domain.model\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath='glove.6B/glove.6B.100d.txt', \n",
    "    use_glove=False,\n",
    "    use_domain_specific=True,\n",
    "    embedding_size=100, \n",
    "    hidden_dim=100, \n",
    "    num_channels=100, \n",
    "    kernel_size=3,\n",
    "    # Training hyper parameter\n",
    "    seed=1337, \n",
    "    learning_rate=0.001, \n",
    "    dropout_p=0.1, \n",
    "    batch_size=128, \n",
    "    num_epochs=100, \n",
    "    early_stopping_criteria=5, \n",
    "    # Runtime option\n",
    "    cuda=True, \n",
    "    catch_keyboard_interrupt=True, \n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    train_proportion=0.7,\n",
    "    val_proportion=0.1,\n",
    "    test_proportion=0.2\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "if args.use_glove == True:\n",
    "    if args.expand_filepaths_to_save_dir:\n",
    "        args.vectorizer_file = os.path.join(args.save_dir_10_words_pretrained,\n",
    "                                            args.vectorizer_file)\n",
    "\n",
    "        args.model_state_file = os.path.join(args.save_dir_10_words_pretrained,\n",
    "                                            args.model_state_file)\n",
    "\n",
    "        # Check CUDA\n",
    "        if not torch.cuda.is_available():\n",
    "            args.cuda = False\n",
    "\n",
    "        args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "            \n",
    "        print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "\n",
    "        # Set seed for reproducibility\n",
    "        set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "        # handle dirs\n",
    "        handle_dirs(args.save_dir_10_words_pretrained)\n",
    "\n",
    "if args.use_domain_specific == True:\n",
    "    if args.expand_filepaths_to_save_dir:\n",
    "        args.vectorizer_file = os.path.join(args.save_dir_10_words_domain_speicific,\n",
    "                                            args.vectorizer_file)\n",
    "\n",
    "        args.model_state_file = os.path.join(args.save_dir_10_words_domain_speicific,\n",
    "                                            args.model_state_file)\n",
    "\n",
    "        # Check CUDA\n",
    "        if not torch.cuda.is_available():\n",
    "            args.cuda = False\n",
    "\n",
    "        args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "            \n",
    "        print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "\n",
    "        # Set seed for reproducibility\n",
    "        set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "        # handle dirs\n",
    "        handle_dirs(args.save_dir_10_words_domain_speicific)\n",
    "\n",
    "if args.use_glove == False and args.use_domain_specific == False:\n",
    "    if args.expand_filepaths_to_save_dir:\n",
    "        args.vectorizer_file = os.path.join(args.save_dir_10_word_none_embedding,\n",
    "                                            args.vectorizer_file)\n",
    "\n",
    "        args.model_state_file = os.path.join(args.save_dir_10_word_none_embedding,\n",
    "                                            args.model_state_file)\n",
    "\n",
    "        # Check CUDA\n",
    "        if not torch.cuda.is_available():\n",
    "            args.cuda = False\n",
    "\n",
    "        args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "            \n",
    "        print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "\n",
    "        # Set seed for reproducibility\n",
    "        set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "        # handle dirs\n",
    "        handle_dirs(args.save_dir_10_word_none_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"Add a list of tokens into the Vocabulary\n",
    "        \n",
    "        Args:\n",
    "            tokens (list): a list of string tokens\n",
    "        Returns:\n",
    "            indices (list): a list of indices corresponding to the tokens\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings, num_channels, \n",
    "                 hidden_dim, num_classes, dropout_p, \n",
    "                 pretrained_embeddings=None, padding_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_size (int): size of the embedding vectors\n",
    "            num_embeddings (int): number of embedding vectors\n",
    "            filter_width (int): width of the convolutional kernels\n",
    "            num_channels (int): number of convolutional kernels per layer\n",
    "            hidden_dim (int): the size of the hidden dimension\n",
    "            num_classes (int): the number of classes in classification\n",
    "            dropout_p (float): a dropout parameter \n",
    "            pretrained_embeddings (numpy.array): previously trained word embeddings default is None \n",
    "            padding_idx (int): an index representing a null position\n",
    "        \"\"\"\n",
    "        super(CNNClassifier, self).__init__()\n",
    "\n",
    "        if pretrained_embeddings is None:\n",
    "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
    "                                    num_embeddings=num_embeddings,\n",
    "                                    padding_idx=padding_idx)        \n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
    "                                    num_embeddings=num_embeddings,\n",
    "                                    padding_idx=padding_idx,\n",
    "                                    _weight=pretrained_embeddings)\n",
    "        \n",
    "            \n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_size, \n",
    "                   out_channels=num_channels, kernel_size=args.kernel_size),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                   kernel_size=args.kernel_size, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                   kernel_size=args.kernel_size),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self._dropout_p = dropout_p\n",
    "        self.fc1 = nn.Linear(num_channels, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "            x_in.shape should be (batch, dataset._max_seq_length)\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
    "        \"\"\"\n",
    "        \n",
    "        # embed and permute so features are channels\n",
    "        x_embedded = self.emb(x_in).permute(0, 2, 1)\n",
    "\n",
    "        features = self.convnet(x_embedded)\n",
    "\n",
    "        # average and remove the extra dimension\n",
    "        remaining_size = features.size(dim=2)\n",
    "        features = F.avg_pool1d(features, remaining_size).squeeze(dim=2)\n",
    "        features = F.dropout(features, p=self._dropout_p)\n",
    "        \n",
    "        # mlp classifier\n",
    "        intermediate_vector = F.relu(F.dropout(self.fc1(features), p=self._dropout_p))\n",
    "        prediction_vector = self.fc2(intermediate_vector)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTfidfVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"    \n",
    "    def __init__(self, job_descript_tfidf_vocab, job_type_vocab):\n",
    "        self.job_descript_tfidf_vocab = job_descript_tfidf_vocab\n",
    "        self.job_type_vocab = job_type_vocab\n",
    "\n",
    "    def vectorize(self, tfidf, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            title (str): the string of words separated by a space\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        Returns:\n",
    "            the vetorized title (numpy.array)\n",
    "        \"\"\"\n",
    "        indices = [self.job_descript_tfidf_vocab.begin_seq_index]\n",
    "        indices.extend(self.job_descript_tfidf_vocab.lookup_token(token) \n",
    "                       for token in tfidf.split(\" \"))\n",
    "        indices.append(self.job_descript_tfidf_vocab.end_seq_index)\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.job_descript_tfidf_vocab.mask_index\n",
    "\n",
    "        return out_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, tfidf_df, cutoff=25):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            tfidf_df (pandas.DataFrame): the target dataset\n",
    "            cutoff (int): frequency threshold for including in Vocabulary \n",
    "        Returns:\n",
    "            an instance of the CNNTfidfVectorizer\n",
    "        \"\"\"\n",
    "        job_type_vocab = Vocabulary()        \n",
    "        for job_type in sorted(set(tfidf_df.job_type)):\n",
    "            job_type_vocab.add_token(job_type)\n",
    "\n",
    "        word_counts = Counter()\n",
    "        for tfidf in tfidf_df.tfidf:\n",
    "            for token in tfidf.split(\" \"):\n",
    "                if token not in string.punctuation:\n",
    "                    word_counts[token] += 1\n",
    "        \n",
    "        job_descript_tfidf_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                job_descript_tfidf_vocab.add_token(word)\n",
    "        \n",
    "        return cls(job_descript_tfidf_vocab, job_type_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        job_descript_tfidf_vocab = \\\n",
    "            SequenceVocabulary.from_serializable(contents['job_descript_tfidf_vocab'])\n",
    "        job_type_vocab =  \\\n",
    "            Vocabulary.from_serializable(contents['job_type_vocab'])\n",
    "\n",
    "        return cls(title_vocab=job_descript_tfidf_vocab, category_vocab=job_type_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'job_descript_tfidf_vocab': self.job_descript_tfidf_vocab.to_serializable(),\n",
    "                'job_type_vocab': self.job_type_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTfidfDataset(Dataset):\n",
    "    def __init__(self, tfidf_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tfidf_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (CNNTfidfVectorizer): vectorizer instatiated from dataset\n",
    "        \"\"\"\n",
    "        self.tfidf_df = tfidf_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        # +1 if only using begin_seq, +2 if using both begin and end seq tokens\n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, tfidf_df.tfidf)) + 2\n",
    "        \n",
    "\n",
    "        self.train_df = self.tfidf_df[self.tfidf_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.tfidf_df[self.tfidf_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.tfidf_df[self.tfidf_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "        # Class weights\n",
    "        class_counts = tfidf_df.job_type.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.job_type_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, tfidf_df):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        \n",
    "        Args:\n",
    "            tfidf_df (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of CNNTfidfDataset\n",
    "        \"\"\"\n",
    "        tfidf_df = pd.read_csv(tfidf_df)\n",
    "        train_tfidf_df = tfidf_df[tfidf_df.split=='train']\n",
    "        return cls(tfidf_df, CNNTfidfVectorizer.from_dataframe(train_tfidf_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, tfidf_csv, vectorizer_filepath):\n",
    "        \"\"\"Load dataset and the corresponding vectorizer. \n",
    "        Used in the case in the vectorizer has been cached for re-use\n",
    "        \n",
    "        Args:\n",
    "            tfidf_csv (str): location of the dataset\n",
    "            vectorizer_filepath (str): location of the saved vectorizer\n",
    "        Returns:\n",
    "            an instance of CNNTfidfDataset\n",
    "        \"\"\"\n",
    "        tfidf_csv = pd.read_csv(tfidf_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(tfidf_csv, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"a static method for loading the vectorizer from file\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
    "        Returns:\n",
    "            an instance of CNNTfidfVectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return CNNTfidfVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"saves the vectorizer to disk using json\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        job_descript_vector = \\\n",
    "            self._vectorizer.vectorize(row.tfidf, self._max_seq_length)\n",
    "\n",
    "        job_type_index = \\\n",
    "            self._vectorizer.job_type_vocab.lookup_token(row.job_type)\n",
    "\n",
    "        return {'x_data': job_descript_vector,\n",
    "                'y_target': job_type_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CNNTfidfDataset.load_dataset_and_make_vectorizer(args.output)\n",
    "#dataset.save_vectorizer(args.vectorizer_file)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=2,\n",
    "                            shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                batch_size=args.batch_size, \n",
    "                                device=args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_from_file(glove_filepath):\n",
    "    \"\"\"\n",
    "    Load the GloVe embeddings \n",
    "    \n",
    "    Args:\n",
    "        glove_filepath (str): path to the glove embeddings file \n",
    "    Returns:\n",
    "        word_to_index (dict), embeddings (numpy.ndarary)\n",
    "    \"\"\"\n",
    "\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    with open(glove_filepath, encoding=\"utf8\") as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            line = line.split(\" \") # each line: word num1 num2 ...\n",
    "            word_to_index[line[0]] = index # word = line[0] \n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "    return word_to_index, np.stack(embeddings)\n",
    "\n",
    "def make_embedding_matrix(glove_filepath, words):\n",
    "    \"\"\"\n",
    "    Create embedding matrix for a specific set of words.\n",
    "    \n",
    "    Args:\n",
    "        glove_filepath (str): file path to the glove embeddigns\n",
    "        words (list): list of words in the dataset\n",
    "    \"\"\"\n",
    "    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n",
    "    embedding_size = glove_embeddings.shape[1]\n",
    "    \n",
    "    final_embeddings = np.zeros((len(words), embedding_size))\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "\n",
    "    return final_embeddings\n",
    "    \n",
    "def make_embedding_matrix_domain_speicific(embedding_filepath, words):\n",
    "    \"\"\"\n",
    "    Create embedding matrix for a specific set of words.\n",
    "\n",
    "    Args:\n",
    "        embedding_filepath (str): file path to the word2vec embeddings\n",
    "        words (list): list of words in the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    domain_embeddings = Word2Vec.load(embedding_filepath)\n",
    "    word_to_idx = domain_embeddings.wv.key_to_index\n",
    "    domain_vector = domain_embeddings.wv.get_normed_vectors()\n",
    "    embedding_size = domain_vector.shape[1]\n",
    "\n",
    "    final_embeddings = np.zeros((len(words), embedding_size))\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i, :] = domain_vector[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using domain specific embeddings\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    # training from a checkpoint\n",
    "    dataset = CNNTfidfDataset.load_dataset_and_load_vectorizer(args.output,\n",
    "                                                           args.vectorizer_file)\n",
    "else:\n",
    "    # create dataset and vectorizer\n",
    "    dataset = CNNTfidfDataset.load_dataset_and_make_vectorizer(args.output)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# Use GloVe or randomly initialized embeddings\n",
    "if args.use_glove == True and args.use_domain_specific == False:\n",
    "    words = vectorizer.job_descript_tfidf_vocab._token_to_idx.keys()\n",
    "    embeddings = make_embedding_matrix(glove_filepath=args.glove_filepath, \n",
    "                                       words=words)\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "\n",
    "elif args.use_domain_specific == True and args.use_glove == False:\n",
    "    words = vectorizer.job_descript_tfidf_vocab._token_to_idx.keys()\n",
    "    embeddings = make_embedding_matrix_domain_speicific(embedding_filepath=args.domain_model_dir, \n",
    "                                       words=words)\n",
    "    print(\"Using domain specific embeddings\")\n",
    "\n",
    "else:\n",
    "    embeddings = None\n",
    "    print(\"Using none embeddings\")\n",
    "\n",
    "classifier = CNNClassifier(embedding_size=args.embedding_size, \n",
    "                            num_embeddings=len(vectorizer.job_descript_tfidf_vocab),\n",
    "                            num_channels=args.num_channels,\n",
    "                            hidden_dim=args.hidden_dim, \n",
    "                            num_classes=len(vectorizer.job_type_vocab), \n",
    "                            dropout_p=args.dropout_p,\n",
    "                            pretrained_embeddings=embeddings,\n",
    "                            padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "    \n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # compute the output\n",
    "            y_pred =  classifier(batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred =  classifier(batch_dict['x_data'])\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6523079535235531\n",
      "Test Accuracy: 58.83152173913043\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {}\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried the CNN with three different embeddings, none, pretrained and domain specific and the domain specific gave the highest accuracy using learning_rate=0.001, dropout_p=0.1, batch_size=128, early_stopping_criteria=5, embedding_size=100, hidden_dim=100, num_channels=100, and kernel_size=3 with the following results:\n",
    "\n",
    "None embeddings:\n",
    "\n",
    "Test loss: 0.6523476489212201\n",
    "\n",
    "Test Accuracy: 58.86548913043478\n",
    "\n",
    "Pretrained Embedding:\n",
    "\n",
    "Test loss: 0.6516470131666766\n",
    "\n",
    "Test Accuracy: 58.74660326086957\n",
    "\n",
    "Domain Specific Embedding:\n",
    "\n",
    "Test loss: 0.651664490285127\n",
    "\n",
    "Test Accuracy: 58.93342391304348\n",
    "\n",
    "Then we tried different parameters and try to get better accuracy, we run each test three times and find the average.\n",
    "\n",
    "parameters kept: batch_size=128, embedding_size=100, early_stopping_criteria=5, kernel_size=3\n",
    "\n",
    "| learning_rate | dropout_p | hidden_dim | num_channel | kernel_size | loss               | Accuracy           |\n",
    "|---------------|-----------|------------|-------------|-------------|--------------------|--------------------|\n",
    "| 0.001         | 0.1       | 100        | 100         | 3           | 0.651664490285127  | 58.93342391304348  |\n",
    "| 0.01          | 0.1       | 100        | 100         | 3           | 0.6539218062939854 | 58.271059782608695 |\n",
    "| 0.0001        | 0.1       | 100        | 100         | 3           | 0.6522674068160679 | 58.882472826086946 |\n",
    "| 0.001         | 0.01      | 100        | 100         | 3           | 0.6520292020362357 | 58.729619565217384 |\n",
    "| 0.001         | 0.3       | 100        | 100         | 3           | 0.6536173535429914 | 58.816929347826088 |\n",
    "| 0.001         | 0.4       | 100        | 100         | 3           | 0.6538983091064121 | 58.86548913043479  |\n",
    "| 0.001         | 0.6       | 100        | 100         | 3           | 0.6561724284420842 | 58.83152173913044  |\n",
    "| 0.001         | 0.4       | 150        | 100         | 3           | 0.6529110907644466 | 58.72395833333333  |\n",
    "| 0.001         | 0.4       | 150        | 100         | 3           | 0.6529110907644466 | 58.72395833333333  |\n",
    "| 0.001         | 0.4       | 300        | 300         | 4           | 0.6522181824497554 | 58.89945652173913  |\n",
    "\n",
    "As shown above changing those parameter does not improve the accuracy by much\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(tfidf, classifier, vectorizer, max_length):\n",
    "    \"\"\"Predict a News category for a new title\n",
    "    \n",
    "    Args:\n",
    "        title (str): a raw title string\n",
    "        classifier (CNNtfidfClassifier): an instance of the trained classifier\n",
    "        vectorizer (CNNTfidfVectorizer): the corresponding vectorizer\n",
    "        max_length (int): the max sequence length\n",
    "            Note: CNNs are sensitive to the input data tensor size. \n",
    "                  This ensures to keep it the same size as the training data\n",
    "    \"\"\"\n",
    "    tfidf = preprocess_text(tfidf)\n",
    "    vectorized_title = \\\n",
    "        torch.tensor(vectorizer.vectorize(tfidf, vector_length=max_length))\n",
    "    result = classifier(vectorized_title.unsqueeze(0), apply_softmax=True)\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    predict_job_type = vectorizer.job_type_vocab.lookup_index(indices.item())\n",
    "\n",
    "    return {'job_type': predict_job_type, \n",
    "            'probability': probability_values.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples():\n",
    "    samples = {}\n",
    "    for cat in dataset.val_df.job_type.unique():\n",
    "        samples[cat] = dataset.val_df.tfidf[dataset.val_df.job_type==cat].tolist()[:5]\n",
    "    return samples\n",
    "\n",
    "val_samples = get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Category: Full Time\n",
      "==============================\n",
      "Prediction: Other (p=0.57)\n",
      "\t + Sample: kbs kaplan students business student domestic counselling accounting helping international\n",
      "Prediction: Full Time (p=0.61)\n",
      "\t + Sample: mammoet wind australian advisor quality renewables turbines assurance turbine farm\n",
      "Prediction: Full Time (p=0.61)\n",
      "\t + Sample: mammoet wind australian advisor quality renewables turbines assurance turbine farm\n",
      "Prediction: Full Time (p=0.79)\n",
      "\t + Sample: startrack collate freight analyst post unrivalled range logistics customers country\n",
      "Prediction: Full Time (p=0.73)\n",
      "\t + Sample: amp fm facilities leadership forecasting contract compliance corporate performance financial\n",
      "------------------------------\n",
      "\n",
      "True Category: Other\n",
      "==============================\n",
      "Prediction: Full Time (p=0.62)\n",
      "\t + Sample: automation schneider electric place top process work management segreatpeople lifeison\n",
      "Prediction: Full Time (p=0.51)\n",
      "\t + Sample: pest anticimex flick control australias hygiene although qualificationscredentials onteamwork 35branches\n",
      "Prediction: Other (p=0.51)\n",
      "\t + Sample: mondelz eyecatching considered merchandising placement international per countrys townsville maximum\n",
      "Prediction: Other (p=0.57)\n",
      "\t + Sample: kbs kaplan students business student domestic counselling accounting helping international\n",
      "Prediction: Other (p=0.56)\n",
      "\t + Sample: kbs kaplan students business student domestic counselling accounting helping international\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier.to(\"cpu\")\n",
    "\n",
    "for truth, sample_group in val_samples.items():\n",
    "    print(f\"True Category: {truth}\")\n",
    "    print(\"=\"*30)\n",
    "    for sample in sample_group:\n",
    "        prediction = predict_category(sample, classifier, \n",
    "                                      vectorizer, dataset._max_seq_length + 1)\n",
    "        print(\"Prediction: {} (p={:0.2f})\".format(prediction['job_type'],\n",
    "                                                  prediction['probability']))\n",
    "        print(\"\\t + Sample: {}\".format(sample))\n",
    "    print(\"-\"*30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above shows the true category from the predicted for each job type and in this case there are two job type such as full time and other. The p values specifies the confidence probability that it is true on the prediction and the probablities above are not high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Conv1d based model using the job description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of this section is to build a cnn model to classify the job type binary classification using the job description collumn which should provide better classification as it has more unique data sets which can make it easier to seperate them into the types.\n",
    "\n",
    "To swap between the word embedding set the different args in the following cells:\n",
    "\n",
    "onehotencoder: use_glove=False, use_domain_specific=False,\n",
    "\n",
    "pretrained: use_glove=True, use_domain_specific=False\n",
    "\n",
    "domain_specific: use_glove=False, use_domain_specific=True\n",
    "\n",
    "The models are saved in the specified folder which are shown in the cell below.\n",
    "\n",
    "The model parameters used below will follows with the previous cnn model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    data_set=\"cleaned_data_new.csv\",\n",
    "    vectorizer_file=\"cnn_vectorizer.json\",\n",
    "    model_state_file=\"cnn_model.pth\",\n",
    "    output=\"output.csv\",\n",
    "    save_dir_descript_one_hot=\"model_storage\\cnn_descript_onehot\",\n",
    "    save_dir_descript_pretrained=\"model_storage\\cnn_descript_pretrained\",\n",
    "    save_dir_descript_domain_specific=\"model_storage\\cnn_descript_domain_specific\",\n",
    "    domain_model_dir=\"model_storage\\domain_model\\domain.model\",\n",
    "    domain_file_name=\"domain.model\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath='glove.6B/glove.6B.100d.txt', \n",
    "    use_glove=False,\n",
    "    use_domain_specific=True,\n",
    "    embedding_size=100, \n",
    "    hidden_dim=100, \n",
    "    num_channels=100, \n",
    "    kernel_size=3,\n",
    "    # Training hyper parameter\n",
    "    seed=1337, \n",
    "    learning_rate=0.001, \n",
    "    dropout_p=0.1, \n",
    "    batch_size=128, \n",
    "    num_epochs=100, \n",
    "    early_stopping_criteria=5, \n",
    "    # Runtime option\n",
    "    cuda=True, \n",
    "    catch_keyboard_interrupt=True, \n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    train_proportion=0.7,\n",
    "    val_proportion=0.1,\n",
    "    test_proportion=0.2\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "if args.use_glove == True:\n",
    "    if args.expand_filepaths_to_save_dir:\n",
    "        args.vectorizer_file = os.path.join(args.save_dir_descript_pretrained,\n",
    "                                            args.vectorizer_file)\n",
    "\n",
    "        args.model_state_file = os.path.join(args.save_dir_descript_pretrained,\n",
    "                                            args.model_state_file)\n",
    "\n",
    "        # Check CUDA\n",
    "        if not torch.cuda.is_available():\n",
    "            args.cuda = False\n",
    "\n",
    "        args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "            \n",
    "        print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "\n",
    "        # Set seed for reproducibility\n",
    "        set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "        # handle dirs\n",
    "        handle_dirs(args.save_dir_descript_pretrained)\n",
    "\n",
    "if args.use_domain_specific == True:\n",
    "    if args.expand_filepaths_to_save_dir:\n",
    "        args.vectorizer_file = os.path.join(args.save_dir_descript_domain_specific,\n",
    "                                            args.vectorizer_file)\n",
    "\n",
    "        args.model_state_file = os.path.join(args.save_dir_descript_domain_specific,\n",
    "                                            args.model_state_file)\n",
    "\n",
    "        # Check CUDA\n",
    "        if not torch.cuda.is_available():\n",
    "            args.cuda = False\n",
    "\n",
    "        args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "            \n",
    "        print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "\n",
    "        # Set seed for reproducibility\n",
    "        set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "        # handle dirs\n",
    "        handle_dirs(args.save_dir_descript_domain_specific)\n",
    "\n",
    "if args.use_glove == False and args.use_domain_specific == False:\n",
    "    if args.expand_filepaths_to_save_dir:\n",
    "        args.vectorizer_file = os.path.join(args.save_dir_descript_one_hot,\n",
    "                                            args.vectorizer_file)\n",
    "\n",
    "        args.model_state_file = os.path.join(args.save_dir_descript_one_hot,\n",
    "                                            args.model_state_file)\n",
    "    \n",
    "\n",
    "        # Check CUDA\n",
    "        if not torch.cuda.is_available():\n",
    "            args.cuda = False\n",
    "\n",
    "        args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "            \n",
    "        print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "\n",
    "        # Set seed for reproducibility\n",
    "        set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "        # handle dirs\n",
    "        handle_dirs(args.save_dir_descript_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDescriptVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"    \n",
    "    def __init__(self, job_descript_vocab, job_type_vocab):\n",
    "        self.job_descript_vocab = job_descript_vocab\n",
    "        self.job_type_vocab = job_type_vocab\n",
    "\n",
    "    def vectorize(self, job_description_clean, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            job_description_clean (str): the string of words separated by a space\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        Returns:\n",
    "            the vetorized job_description_clean (numpy.array)\n",
    "        \"\"\"\n",
    "        indices = [self.job_descript_vocab.begin_seq_index]\n",
    "        indices.extend(self.job_descript_vocab.lookup_token(token) \n",
    "                       for token in job_description_clean.split(\" \"))\n",
    "        indices.append(self.job_descript_vocab.end_seq_index)\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.job_descript_vocab.mask_index\n",
    "\n",
    "        return out_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, job_descript_df, cutoff=25):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            job_descript_df (pandas.DataFrame): the target dataset\n",
    "            cutoff (int): frequency threshold for including in Vocabulary \n",
    "        Returns:\n",
    "            an instance of the CNNDescriptVectorizer\n",
    "        \"\"\"\n",
    "        job_type_vocab = Vocabulary()        \n",
    "        for job_type in sorted(set(job_descript_df.job_type)):\n",
    "            job_type_vocab.add_token(job_type)\n",
    "\n",
    "        word_counts = Counter()\n",
    "        for job_description_clean in job_descript_df.job_description_clean:\n",
    "            for token in job_description_clean.split(\" \"):\n",
    "                if token not in string.punctuation:\n",
    "                    word_counts[token] += 1\n",
    "        \n",
    "        job_descript_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                job_descript_vocab.add_token(word)\n",
    "        \n",
    "        return cls(job_descript_vocab, job_type_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        job_descript_vocab = \\\n",
    "            SequenceVocabulary.from_serializable(contents['job_descript_vocab'])\n",
    "        job_type_vocab =  \\\n",
    "            Vocabulary.from_serializable(contents['job_type_vocab'])\n",
    "\n",
    "        return cls(title_vocab=job_descript_vocab, category_vocab=job_type_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'job_descript_vocab': self.job_descript_vocab.to_serializable(),\n",
    "                'job_type_vocab': self.job_type_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDescriptDataset(Dataset):\n",
    "    def __init__(self, job_descript_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            job_descript_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (CNNDescriptVectorizer): vectorizer instatiated from dataset\n",
    "        \"\"\"\n",
    "        self.job_descript_df = job_descript_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        # +1 if only using begin_seq, +2 if using both begin and end seq tokens\n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, job_descript_df.job_description_clean)) + 2\n",
    "        \n",
    "\n",
    "        self.train_df = self.job_descript_df[self.job_descript_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.job_descript_df[self.job_descript_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.job_descript_df[self.job_descript_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "        # Class weights\n",
    "        class_counts = job_descript_df.job_type.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.job_type_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, job_descript_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        \n",
    "        Args:\n",
    "            job_descript_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of CNNDescriptDataset\n",
    "        \"\"\"\n",
    "        job_descript_df = pd.read_csv(job_descript_csv)\n",
    "        train_job_descript_df = job_descript_df[job_descript_df.split=='train']\n",
    "        return cls(job_descript_df, CNNDescriptVectorizer.from_dataframe(train_job_descript_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, job_descript_csv, vectorizer_filepath):\n",
    "        \"\"\"Load dataset and the corresponding vectorizer. \n",
    "        Used in the case in the vectorizer has been cached for re-use\n",
    "        \n",
    "        Args:\n",
    "            job_descript_csv (str): location of the dataset\n",
    "            vectorizer_filepath (str): location of the saved vectorizer\n",
    "        Returns:\n",
    "            an instance of CNNDescriptDataset\n",
    "        \"\"\"\n",
    "        job_descript_csv = pd.read_csv(job_descript_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(job_descript_csv, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"a static method for loading the vectorizer from file\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
    "        Returns:\n",
    "            an instance of CNNDescriptVectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return CNNDescriptVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"saves the vectorizer to disk using json\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        job_descript_vector = \\\n",
    "            self._vectorizer.vectorize(row.job_description_clean, self._max_seq_length)\n",
    "\n",
    "        job_type_index = \\\n",
    "            self._vectorizer.job_type_vocab.lookup_token(row.job_type)\n",
    "\n",
    "        return {'x_data': job_descript_vector,\n",
    "                'y_target': job_type_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CNNDescriptDataset.load_dataset_and_make_vectorizer(args.output)\n",
    "#dataset.save_vectorizer(args.vectorizer_file)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=2,\n",
    "                            shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                batch_size=args.batch_size, \n",
    "                                device=args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns: a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using domain specific embeddings\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    # training from a checkpoint\n",
    "    dataset = CNNDescriptDataset.load_dataset_and_load_vectorizer(args.output,\n",
    "                                                           args.vectorizer_file)\n",
    "else:\n",
    "    # create dataset and vectorizer\n",
    "    dataset = CNNDescriptDataset.load_dataset_and_make_vectorizer(args.output)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# Use GloVe or randomly initialized embeddings\n",
    "if args.use_glove:\n",
    "    words = vectorizer.job_descript_vocab._token_to_idx.keys()\n",
    "    embeddings = make_embedding_matrix(glove_filepath=args.glove_filepath, \n",
    "                                       words=words)\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "\n",
    "elif args.use_domain_specific:\n",
    "    words = vectorizer.job_descript_vocab._token_to_idx.keys()\n",
    "    embeddings = make_embedding_matrix_domain_speicific(embedding_filepath=args.domain_model_dir, \n",
    "                                       words=words)\n",
    "    print(\"Using domain specific embeddings\")\n",
    "\n",
    "else:\n",
    "    embeddings = None\n",
    "    print(\"Using none embeddings\")\n",
    "\n",
    "classifier = CNNClassifier(embedding_size=args.embedding_size, \n",
    "                            num_embeddings=len(vectorizer.job_descript_vocab),\n",
    "                            num_channels=args.num_channels,\n",
    "                            hidden_dim=args.hidden_dim, \n",
    "                            num_classes=len(vectorizer.job_type_vocab), \n",
    "                            dropout_p=args.dropout_p,\n",
    "                            pretrained_embeddings=embeddings,\n",
    "                            padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "    \n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # compute the output\n",
    "            y_pred =  classifier(batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred =  classifier(batch_dict['x_data'])\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.705501051376695;\n",
      "Test Accuracy: 82.08220108695653\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None vectoriser:\n",
    "\n",
    "Test loss: 0.7476352446753047\n",
    "\n",
    "Test Accuracy: 82.42187499999999\n",
    "\n",
    "Pretrained Embedding:\n",
    "\n",
    "Test loss: 0.5494730355946914\n",
    "\n",
    "Test Accuracy: 82.33695652173911\n",
    "\n",
    "Domain Specific Embedding:\n",
    "\n",
    "Test loss: 0.705501051376695\n",
    "\n",
    "Test Accuracy: 82.08220108695653\n",
    "\n",
    "This shows that using the whole description is better to classify than using the tfidf words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(job_description_clean, classifier, vectorizer, max_length):\n",
    "    \"\"\"Predict a job category for a new title\n",
    "    \n",
    "    Args:\n",
    "        title (str): a raw title string\n",
    "        classifier (CNNClassifier): an instance of the trained classifier\n",
    "        vectorizer (CNNDescriptVectorizer): the corresponding vectorizer\n",
    "        max_length (int): the max sequence length\n",
    "            Note: CNNs are sensitive to the input data tensor size. \n",
    "                  This ensures to keep it the same size as the training data\n",
    "    \"\"\"\n",
    "    job_description_clean = preprocess_text(job_description_clean)\n",
    "    vectorized_title = \\\n",
    "        torch.tensor(vectorizer.vectorize(job_description_clean, vector_length=max_length))\n",
    "    result = classifier(vectorized_title.unsqueeze(0), apply_softmax=True)\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    predict_job_type = vectorizer.job_type_vocab.lookup_index(indices.item())\n",
    "\n",
    "    return {'job_type': predict_job_type, \n",
    "            'probability': probability_values.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples():\n",
    "    samples = {}\n",
    "    for cat in dataset.val_df.job_type.unique():\n",
    "        samples[cat] = dataset.val_df.job_description_clean[dataset.val_df.job_type==cat].tolist()[:5]\n",
    "    return samples\n",
    "\n",
    "val_samples = get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Category: Full Time\n",
      "==============================\n",
      "Prediction: Full Time (p=0.54)\n",
      "\t + Sample:  located in western sydney our client is looking for an early childhood teacher that is energetic and dedicated to their role as a teacher their ideal teacher will be innovative and willing to deliver a responsive service that is committed to inclusive practices that support families cultural socioeconomic and individual situations what do you need to apply bachelor of early childhood teaching qualification acecqa approved nsw working with children check teacher accreditation number nesa approved child protection training first aid asthma and anaphylaxis certificate why apply for this role this particular organisation is recognised for their professionalism and high quality of care the organisation is committed to meeting the needs of their employees this includes having competitive salaries and opportunity for continuous development \n",
      "Prediction: Full Time (p=0.99)\n",
      "\t + Sample:  on off you should hold a degree or equivalent in spatial science and have the following a minimum years surveying experience in civil construction open manual drivers license experience with autocad experience with d d field is preferred but not mandatory if you are looking to progress your survey career and you believe you are suitable for this position we welcome your application \n",
      "Prediction: Full Time (p=1.00)\n",
      "\t + Sample:  this role represents a great opportunity for an experienced survey party leaderto secure their next permanent role in the booming melbourne market our client is a well established engineering and survey firm based in melbournethey have enjoyed continued growthfor the past decade and the bulk of their work is in the land developmentsector as a result this company has an urgent requirement for an experiencedparty leaderto join their team working in the field and reporting to a highly accomplished survey manager you will be engaged on some of victorias most prolific land development projectsyou will need to demonstrate experience working on cadastral projects in victoria this will be coupled with excellent communication skills and the proven ability to work as part of a team we can negotiate a great salary package on your behalf with our client as well as market leading employment conditions you will have a great attitude to working in a team environment and have a positive attitude at all times in exchange for your commitment to this long term role you will be rewarded with a market leading salary and fantastic employment conditions to apply or discuss this role in further detail please contact david moon on or submit your cv via the link below \n",
      "Prediction: Full Time (p=1.00)\n",
      "\t + Sample:  the lowdown you will be very comfortable in a rapidly growing tech company that brings the real world to you with the most frequentlyupdated location content in the world you will be able to roll up your sleeves and do whats necessary to succeed you will thrive in a flexible and diverse environment that craves an entrepreneurial spirit and is looking for leaders to accelerate the success we are achieving you crave an environment that wants your input to improve the way things get done and do so with a nofear cando attitude as a strategic account manageryou willacquire new target accounts and expand relationships withnearmapsstrategic customers you will be responsible for achieving sales quota and ensuring strategic account objectives such as customer satisfaction nps are achieved a typical day for you may look like this manage the existing business pipeline of renewals ensuring they are achieved through value add activities make your own leads dont wait for them to come to you work your way through strategic prospect and existing customer accounts to help orchestrate large value solution propositions and secure deals create territory and account plans to bring in new nearmap customers and build relationships with our existing customers talk about how nearmaps unique value proposition benefits customers be particular in correctly recording into our systems develop highly effective relationships within the nearmap family monitor your own performance by measuring your achievements through revenue targets use your entrepreneurial skills to improve and share methods of success with your fellow nearmappers be passionate and represent our amazing company the essentials passion for new business generation years experience in a direct enterprise account selling environment software or saas sales experience is a plus be strategic solution sales focussed experience in a proven sales methodology eg sandler miller heiman tas spin challenger demonstrated ability to engage and influence clevel executives be a pro at building and executing detailed account plans competence with crm software salesforce a knack to negotiate and close deals an entrepreneurial spirit that drives you to want to try new things own and breathe a customercentric philosophy whats in it for you uncapped commission plan competitive compensation package industry leading disruptive and unique product suite high performing and results driven culture free breakfast healthy snacks and lunches provided fortnightly inhouse massages monthly wellness allowance fun workplace with lots of social events and competitions please apply if you feel you meet our selected criteria and are looking to join a fun and vibrant organisation we look forward to your application \n",
      "Prediction: Full Time (p=1.00)\n",
      "\t + Sample: the client this organisation is one of the top brands in the residential development world with a reputation for delivering sustainable communities in brisbane melbourne the role as the project marketing manager you will report to the head of marketing and will primarily be responsible for the development of marketing strategies and marketing activity your duties will include but will not be limited to orking with senior management to build and implement marketing communications solutions in line with business strategy conveying the company solutions in line with business strategy conveying the company message through managing of the organisations marketing communications program including media coverage contributed articles and press releases recommending newsworthy data and product announcements in line with products marketing managing the effectiveness of all marketing communications activities including the governance of key messages tactics budgets timing and measurement managing the effective internal dissemination of company news announcements marketing event calendars other communications acting as the organisations spokes person where necessary to be successful you must have the following years experience in agency andor marketing roles experience with property andor development essential bachelors degree in marketing pr advertising or equivalent discipline ideal strong analytical and interpretation skills ability to work autonomously and in a team experience in developing implementing marketing plans and campaigns strong budget management experience and business acumen this is a great opportunity to work for a wellestablished international company if successful you will be compensated with a healthy salary package and be able to work with an amazing team in melbourne to apply click on the link below or email a confidential cv in word format only to kimberley hoedemaeckers at khoedemaeckersgoughrecruitmentcomau or call after submitting an application please note that due to high application volumes only short listed candidates will be contacted only australian permanent residents are eligible to apply \n",
      "------------------------------\n",
      "\n",
      "True Category: Other\n",
      "==============================\n",
      "Prediction: Other (p=0.95)\n",
      "\t + Sample:  our client is a large nsw state government department they are currently seeking a junior operational systems coordinator to work with them in a newly created positon for a leading transport project the primary purpose of the role is to coordinate interfaces for the clients certain systems including the electronic ticketing system ets and to ensure relevant technical systems requirements are considered across multiple contracts throughout all stages of the project the initial contract is months with high potential to be extended responsibilities establish a project agreement for the client ensuring a common approach to ticketing issues across multiple contracts facilitate the identification of requirements and design reviews to achieve project objectives oversee progress of delivery and commissioning of ets equipment to ensure project milestones are met review commercial arrangements for delivery through the clients and project agreements to ensure alignment between department contracts determine operating phase requirements and commitments for ongoing operations and maintenance to ensure operating phase interface arrangements and kpis deliver ets related customer service and revenue protection policy objectives facilitate ets related capex budget and opex budget forecasting through tac ensure go to market requirements for opal ticketing including customer information are understood and incorporated into operational readiness processes experience tertiary qualification in either an engineering construction project management business administration or related discipline from a recognised tertiary institution or equivalent experience extensive experience in leading stakeholder interfaces general understanding of public transport ticketing systems throughout the asset lifecycle or comparable systems in a different industry understanding of key commercial considerations in managing the interfaces between stakeholdertechnical requirements and commercial contracts previous experience in a nsw government department will be highly regarded have the ability to work in a team environment while also working autonomously when needed what you need to do now if you would like to be considered for this role please submit your cv in word format to our recruitment consultant gemma fernie please note only shortlisted candidates will be contacted \n",
      "Prediction: Other (p=0.91)\n",
      "\t + Sample:  our client is best known as one of the worlds leading manufacturers of trucks commercial equipment buses and industrial engines they are motived by their core values of quality safety and environmental care they are currently experiencing growth and are seeking qualified heavy vehicle technicians for their site based in chullora this is a rewarding role that will see you servicing and repairing an impressive range of trucks there are various shifts on offer including day start time at either am am or am afternoon start time of pm night start time of pm whats on offer temp to perm and fulltime opportunity on offer for the right person day afternoon and night shift on offer stable roster with set shift time no rotation required generous tool allowance an opportunity to develop your career with a variety of training and development options supportive and familyfriendly team culture focused on diversity and equality a dedicated health and wellbeing program to be successful you must have a valid motor vehicle repairers licence certificate iii in heavy commercial vehicle mechanical technology or equivalent sound heavy vehicle mechanical knowledge team player and support others on site flexible approach to work commitments have a strong understanding of the importance of quality control measures strong safety focus and able to follow procedures if you are seeking a new opportunity with a wellknown reputable branch please apply via the link with a current resume \n",
      "Prediction: Full Time (p=0.98)\n",
      "\t + Sample: about us seafolly is about sharing the spirit of australian summer with women everywhere we are sold in over doors in countries through its own retail and online channels leading retailers and major online sites reporting to the digital amp ecommerce director you will compile all online content to ensure it is accurate and ready to be uploaded onto the new website for both seafolly amp sunburn create and upload seo amp consumer optimized product descriptions to new website upload accurate product meta data information ensure the appropriate associating and categorization mapping of products merchandise categories in alignment with business requirements create relevant landing pages to support digital marketing efforts for launch and ongoing internal promotionsevents create product guides size charts and other web content as needed collaborate crossfunctionally to support internal initiatives you will need knowledge and understanding of demandware andor similar web cms systems experience using ap andor similar erp systems excellent written amp verbal communication skills ability to prioritise a steady workload advanced excel skills benefits exclusive employee benefits and discounts globally recognized iconic australian brand with huge vision and growth plans great work culture in a passionate team environment located in our stunning new industrial chic office enjoy the various surrounding amenities including sydneys best coffee at the grounds of alexandria at seafolly we are super passionate supportive and offer you the freedom to shine apply now and comequotlive life with seafollyquot \n",
      "Prediction: Other (p=0.95)\n",
      "\t + Sample:  what is anzuk anzuk early childhoodis a recruitment agency thatendeavoursto provide childcare services with the finest quality educators in the industry the majority of our consultants have come from an educational background so we understand what it takes to be great in the industry and empathise with those more difficult moments you come up against what is anzuk looking for if you are enthusiastic reliable and have a passion for working in early childhood then we want you you just need to have the following a qualification inearly childhoodeducationcert iii diploma bachelor of early childhood or an acecqa approved equivalent up to datefirst aid cpr asthma and anaphylaxisqualifications awwc check nesaaccreditation ect only professional refereesindustry specific why choose anzuk theanzukteam can guarantee you devotion and passion we want you to gain the most incredible invaluable and enjoyable experiences in sydneys top quality childcare services we believe in building a positive friendly and relationship focused culture whereby we encourage you to reach your potential and thrive off your success our revolutionary software allows you to take control of your own availability and preferences and allows us to be highly efficient consultants if you would like a consultant who isnt just a voice genuinely cares and can spend more time devoted to you then look no further want to join our team contact jess call show number email jessicacshow email feel free to check out our anzuk early childhood facebook page wwwfacebookcomanzukearlychildhood \n",
      "Prediction: Other (p=1.00)\n",
      "\t + Sample:  centre for adult education and foundation studies casual opportunities competitive hourly rates about us the box hill institute group bhi group is composed of box hill institute bhi and the centre for adult education cae we are leading education providers with strong reputations for delivering high quality educational outcomes to maintain our reputation and achieve our strategic goals we need a workforce full of energetic and forward thinking people if you are enthusiastic and resilient with a strong customer and commercial focus then join our team at the bhi group you will work with great people in a challenging and dynamic environment as we work together to continually improve our business about the role the vce chief exam supervisor is responsible for ensuring the fair and proper conduct of vcaa vce external examinations providing an environment where students are able to perform to the best of their ability the key outcomes include set up and day to day operation of the year vce external examinations and to be on site during the examination period prepare and collate all vce response materials and other documents as per vcaa requirements organise a team of assistant exam supervisors during the examination periods conduct briefing sessions for the assistant supervisors in conjunction with the vce manager and the administration coordinator specific vcaa requirements chief supervisors cannot be related to or associated with any student undertaking a vce unit subject in teaching or tutoring any student in a vce unit subject in related to or associated with any person engaged in teaching tutoring or coaching any students undertaking a vce unit subject in or any school personnel engaged in organising or checking vce external assessment materials on behalf of a school in employed at bhicae as a teacher including crt or member of the admin staff your skills industry experience significant relevant experience in supervising exams within a high school or tertiary environment is desirable qualifications o degree in a relevant discipline with relevant work experience or a combination of lesser qualification and significant relevant experience o teaching qualification desirable benefits interested in starting a career with one of australias leading institutes at bhi group we value our people we offer more than just a job we offer benefits to enhance your work life help you grow and celebrate your achievements employment benefits include novated leasing and salary packaging purchased leave on campus permit parking employee discounts via corporate partners discounts on health insurance car rental newspaper subscriptions and more library access digital and online resources discounts on selected onsite services flowers on elgar hair and beauty salons fountains restaurant pets on elgar veterinary clinic and dog grooming counselling support services eap and health and wellbeing program on campus cafeterias elgar lilydale lakeside and nelson campuses discount on a range of cae and box hill institute short courses for staff and their immediate family members on campus childcare lilydale lakeside campus walking distance to restaurant and shopping precincts and easy access to numerous public transport options staff development program study fee reductions for approved internal courses and study support for approved courses reward and recognition programs how to apply like to know more if you have questions about this role please contact sandra wolfe at sandrawolfecaeeduau or on applications close am monday april please note the box hill institute group reserves the right to withdraw an advertised position at any stage you will need to provide a current national police records check and an employee category working with children check you will also need to provide evidence of your entitlement to work in australia and certified copies of your qualifications \n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier.to(\"cpu\")\n",
    "\n",
    "for truth, sample_group in val_samples.items():\n",
    "    print(f\"True Category: {truth}\")\n",
    "    print(\"=\"*30)\n",
    "    for sample in sample_group:\n",
    "        prediction = predict_category(sample, classifier, \n",
    "                                      vectorizer, dataset._max_seq_length + 1)\n",
    "        print(\"Prediction: {} (p={:0.2f})\".format(prediction['job_type'],\n",
    "                                                  prediction['probability']))\n",
    "        print(\"\\t + Sample: {}\".format(sample))\n",
    "    print(\"-\"*30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction above shows promising result as most of the probability are above 0.9 as compared to just using the tfidf as expected since they each have unique descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e4cf2685135efa26385b7503397fd04025f22b378b3367face75219ced6bf2bb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
